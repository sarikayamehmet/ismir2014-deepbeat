## see http://www.red-dove.com/config-doc/

## logger config;
logger : {
	# pattern, see http://docs.python.org/2/library/logging.html#logrecord-attributes
	pattern : '%(relativeCreated)d\t %(levelname)s\t %(funcName)s@%(filename)s:%(lineno)d : %(message)s'
	level : 'debug'
} # end logger

reset_logging : False

experiment_root : '/Users/sstober/git/deepbeat/deepbeat/output/gpu/sda/batch2/batch_50-25-10.d lr0.3 + 0.5 Momentum'
dataset_root : '/Users/sstober/work/datasets/Dan/eeg'			# system=dependent
#output_path : '/Users/sstober/git/deepbeat/deepbeat/output'	# system=dependent

dataset_suffix : '_13goodchannels'

random_seeds : [42,13]
random_seed : 42

subjects : [1]

label_mode : 'rhythm_type'
n_classes : 2

#label_mode : 'rhythm'
#n_classes : 24

## fft params
n_fft : ''
n_freq_bins : 1
spectrum_log_amplitude : True
# spectrum_normalization_mode : 'mean0_std1'
spectrum_normalization_mode : 'linear_0_1'
# spectrum_normalization_mode : 'linear_-1_1'

sample_rate : 100
input_length : 100
hop_size : 25

untie_weights : False
hidden_layers_sizes : [50, 25, 10]
#hidden_layers_sizes : [66, 33, 15] # cost~0.9
#hidden_layers_sizes : [60, 40, 20, 10]
#hidden_layers_sizes : [50, 25]

global_sda : True

pretrain : {
	learning_rate : 0.01
	epochs : 100
	batch_size : 100
	corruption_levels : [0.3, 0.2, 0.1, 0.1]
}

pretrain_finetune : {
	learning_rate : 0.01
	epochs : 100
	batch_size : 100
	corruption_levels : [0.1, 0.1, 0.1, 0.1]
}

layer0_content : $experiment_root+'/sda/sda_layer0_tied.pkl'
layer1_content : $experiment_root+'/sda/sda_layer1_tied.pkl'
layer2_content : $experiment_root+'/sda/sda_layer2_tied.pkl'

output_layer_class : 'deepbeat.pylearn2ext.HingeLoss.HingeLoss'
# output_layer_class : 'pylearn2.models.mlp.Softmax'

max_epochs : 100
termination_patience_epochs : 50

learning_rate : 0.3

lr_exponential_decay_factor : 1.08 	
										# = 1.00004 ** 320 (number of batches per epoch) 
lr_exponential_decay_min_lr : 0.000001

l1_weight_decay_coeffs : [ 0.00001, 0.00001, 0.00001, 0.00001, 0.00001 ]
l2_weight_decay_coeffs : [ 0.00001, 0.00001, 0.00001, 0.00001, 0.00001 ]

learning_rule : Momentum
momentum_start_epoch : 1
momentum_saturate_epoch : 100
momentum_init : 0.5
momentum_final : 0.51

batch_size : 100

input_range : 0.05,

class_prf1_channels : False
confusion_channels : False

only_extract_results : False